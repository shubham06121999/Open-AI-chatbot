{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f2bf6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ae5d6597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text snippets and their labels\n",
    "text_snippets = [\n",
    "    \"This is an amazing product!\", \n",
    "    \"I hate this product!\", \n",
    "    \"This product is okay.\", \n",
    "    \"I love this product!\", \n",
    "    \"This product is terrible.\"\n",
    "]\n",
    "\n",
    "labels = [1,-1,0,1,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b31525",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54b0f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame to store the text snippets and labels\n",
    "df = pd.DataFrame({\"text\": text_snippets, \"label\": labels})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"text_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c6289c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('text_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9f0baee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is an amazing product!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate this product!</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This product is okay.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love this product!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This product is terrible.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text  label\n",
       "0  This is an amazing product!      1\n",
       "1         I hate this product!     -1\n",
       "2        This product is okay.      0\n",
       "3         I love this product!      1\n",
       "4    This product is terrible.     -1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "03f6b959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "count    5.0\n",
       "mean     0.0\n",
       "std      1.0\n",
       "min     -1.0\n",
       "25%     -1.0\n",
       "50%      0.0\n",
       "75%      1.0\n",
       "max      1.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1d2e3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a3b2348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text snippets\n",
    "texts = df.text\n",
    "\n",
    "# Labels (positive or negative sentiment)\n",
    "labels = df.label\n",
    "\n",
    "# Convert text to numerical representation using tokenization\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(padded_sequences, labels, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a6be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a0ce5fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.7225 - accuracy: 0.2500 - val_loss: 0.6845 - val_accuracy: 1.0000\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7153 - accuracy: 0.2500 - val_loss: 0.6857 - val_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7082 - accuracy: 0.5000 - val_loss: 0.6870 - val_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7010 - accuracy: 0.5000 - val_loss: 0.6882 - val_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6894 - val_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6868 - accuracy: 0.5000 - val_loss: 0.6907 - val_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6797 - accuracy: 0.5000 - val_loss: 0.6919 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6726 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6656 - accuracy: 0.5000 - val_loss: 0.6944 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6585 - accuracy: 0.5000 - val_loss: 0.6957 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6515 - accuracy: 0.5000 - val_loss: 0.6970 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6445 - accuracy: 0.5000 - val_loss: 0.6983 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6374 - accuracy: 0.5000 - val_loss: 0.6997 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6304 - accuracy: 0.5000 - val_loss: 0.7010 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6234 - accuracy: 0.5000 - val_loss: 0.7023 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, train_labels, epochs=15, batch_size=32, validation_data=(val_data, val_labels))\n",
    "# Save the model in TensorFlow format\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b807af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TensorFlow SavedModel format\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# Convert the model to TensorFlow SavedModel format\n",
    "model.save('data_model', save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "296d8911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.26.4.tar.gz (55 kB)\n",
      "     ---------------------------------------- 55.6/55.6 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.3-cp39-cp39-win_amd64.whl (323 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\spart\\anaconda3\\lib\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\spart\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\spart\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\spart\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\spart\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\spart\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.11)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\spart\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.8.2-cp39-cp39-win_amd64.whl (56 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\spart\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.5)\n",
      "Building wheels for collected packages: openai\n",
      "  Building wheel for openai (pyproject.toml): started\n",
      "  Building wheel for openai (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai: filename=openai-0.26.4-py3-none-any.whl size=67767 sha256=bb499e5e3cfba5a3783577d9f0886bf37f00a8d94e710f1027c84692fc8fee0e\n",
      "  Stored in directory: c:\\users\\spart\\appdata\\local\\pip\\cache\\wheels\\6c\\15\\7e\\10950b76450328e6997e4319968c5412a28569ccf33b1e4a04\n",
      "Successfully built openai\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.26.4 yarl-1.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5db02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
